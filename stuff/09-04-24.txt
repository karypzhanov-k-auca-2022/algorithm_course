                                            Reward Function

set of Action
set of States/set of observations
Action πolicy (Observation) -> Action

double Reward (Action action, State state, State nextState) {
    double reward = 0;
    if (nextState.isTerminal) {
        reward = 100;
    } else {
        reward = -1;
    }
    return reward;
}

Probability Distribution

double Probability (State s, Action a) {
    double probability = 0;
    if (state.isTerminal) {
        probability = 0;
    } else {
        probability = 1;
    }
    return probability;
}

Rewardsment learning

Markov Decision Processes

Markov Decision Processes (MDPs) are a mathematical framework for modeling
decision-making in situations where outcomes are partly random and partly under the control of a decision maker.
MDPs are useful for studying a wide range of optimization problems solved via dynamic programming and reinforcement learning.

Reversing the MDP
1) Reward Function
2) Set of Action
3) Set of States/set of observations
4) Probability Distribution
5) Discount Factor (γ)

Action πolicy (Observation) -> Action

1) Insertion sort
2) Selection sort
3) Bubble sort
int i
int j
int arr[8] = {5, 2, 4, 6, 1, 3, 2, 7}
for (i = 0; i < 8; i++) {
    for( j = i + 1; j < 8; j++) {
        if (arr[i] > arr[j]) {
            int temp = arr[i];
            arr[i] = arr[j];
            arr[j] = temp;
        }
    }
}
4) Merge sort
5) Quick sort